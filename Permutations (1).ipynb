{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh3wUoF_EeTx"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import json\n",
        "from io import StringIO\n",
        "import requests\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_url = 'https://drive.google.com/uc?id=1MxQI_Z-E9xi2Kv1V-rNwXZKESnAjI4mf'\n",
        "prompts_file = urllib.request.urlopen(file_url)\n",
        "prompts_file = prompts_file.read()\n",
        "prompts_file = prompts_file.decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def download_file(url, destination):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(destination, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(\"File downloaded successfully at\", destination)\n",
        "    else:\n",
        "        print(\"Failed to download file\")\n",
        "\n",
        "def split_file(input_file, output_file1, output_file2):\n",
        "    with open(input_file, 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "    # Determine the splitting point\n",
        "    split_index = len(data) // 2\n",
        "\n",
        "    # Split the data into two parts\n",
        "    part1 = data[:split_index]\n",
        "    part2 = data[split_index:]\n",
        "\n",
        "    # Write the two parts to separate output files\n",
        "    with open(output_file1, 'w') as f1:\n",
        "        f1.writelines(part1)\n",
        "\n",
        "    with open(output_file2, 'w') as f2:\n",
        "        f2.writelines(part2)\n",
        "\n",
        "input_url = 'https://drive.google.com/uc?id=1MxQI_Z-E9xi2Kv1V-rNwXZKESnAjI4mf'\n",
        "output_file1 = 'output1.txt'\n",
        "output_file2 = 'output2.txt'\n",
        "\n",
        "download_file(input_url, 'input.txt')\n",
        "split_file('input.txt', output_file1, output_file2)"
      ],
      "metadata": {
        "id": "ofpZLlXdEi6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read JSON data from input.txt into a DataFrame\n",
        "df = pd.read_json('input.txt', lines=True)\n",
        "\n",
        "# Filter for rows where 'predicate_id' is 'P176'\n",
        "df = df[df['predicate_id'] == 'P449']\n",
        "\n",
        "# Further filter rows where 'sub_label' has two or fewer words\n",
        "df = df[df['sub_label'].apply(lambda x: len(x.split()) <= 2)]\n",
        "\n",
        "# Tokenize the 'sub_label' and 'obj_label'\n",
        "df['sub_label_tokens'] = df['sub_label'].apply(lambda x: set(x.split()))\n",
        "df['obj_label_tokens'] = df['obj_label'].apply(lambda x: set(x.split()))\n",
        "\n",
        "# Create a filter for rows where there is an intersection between 'sub_label' and 'obj_label' tokens\n",
        "intersection_filter = df.apply(lambda row: len(row['sub_label_tokens'].intersection(row['obj_label_tokens'])) == 0, axis=1)\n",
        "\n",
        "# Apply the filter to get the desired rows\n",
        "df = df[intersection_filter]\n",
        "\n",
        "# Optional: Drop the token columns if they are no longer needed\n",
        "df = df.drop(columns=['sub_label_tokens', 'obj_label_tokens'])"
      ],
      "metadata": {
        "id": "MDXtBnSuEpNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ORIGINAL SUBJECT\n",
        "for item in df[\"sub_label\"]:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "wG6gZTRwEp2h",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sentences\n",
        "sentences = [\n",
        "    \" contemporaryimer bombings tainted besieged\",\n",
        "    \" contemporary citizen bombings tainted besieged\",\n",
        "    \" overturnedimer bombings tainted besieged\"\n",
        "]\n",
        "\n",
        "# Iterate over the sentences\n",
        "for sentence in sentences:\n",
        "    # Iterate over the items in the DataFrame column\n",
        "    for item in df[\"sub_label\"]:\n",
        "        print(item + sentence)\n"
      ],
      "metadata": {
        "id": "AtUBtIihlMi7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COUNT TOKENS\n",
        "token_counts = [len(item.split()) for item in df[\"sub_label\"]]\n",
        "\n",
        "# Display the results\n",
        "for count in token_counts:\n",
        "    print(count)"
      ],
      "metadata": {
        "id": "8H7WC8agPccO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FILTERED TO 2 TOKENS\n",
        "for item in df[\"sub_label\"]:\n",
        "    # Split the item into tokens and count them\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        print(item)\n"
      ],
      "metadata": {
        "id": "IaG2Po5wEYrX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SWAPPED\n",
        "for item in df[\"sub_label\"]:\n",
        "    # Split the item into tokens\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        # Swap the tokens' positions\n",
        "        swapped_item = tokens[1] + \" \" + tokens[0]\n",
        "        print(swapped_item)"
      ],
      "metadata": {
        "id": "yZEs3OEUR8vn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "sentences = [\n",
        "    \" stationsbugsCBSbalance sightings\",\n",
        "    \" stationsbugsCBSbalance upheaval\",\n",
        "    \" stationsbugsCBSbalanceclone\"\n",
        "]\n",
        "\n",
        "# List to hold swapped items\n",
        "swapped_items = []\n",
        "\n",
        "# Iterate over each item in the DataFrame's \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    # Split the item into tokens\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        # Swap the tokens' positions\n",
        "        swapped_item = tokens[1] + \" \" + tokens[0]\n",
        "        swapped_items.append(swapped_item)\n",
        "\n",
        "# Iterate over each sentence\n",
        "for sentence in sentences:\n",
        "    # Print each swapped item with the current sentence\n",
        "    for swapped_item in swapped_items:\n",
        "        print(swapped_item + sentence)"
      ],
      "metadata": {
        "id": "lmbDKq8ImLJe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COUNT OBJ TOKENS\n",
        "# Function to count tokens in a given string\n",
        "def has_two_tokens(text):\n",
        "    return len(text.split()) == 2\n",
        "\n",
        "# Apply the filtering condition to the DataFrame\n",
        "filtered_df = df[df[\"sub_label\"].apply(has_two_tokens)]\n",
        "\n",
        "# Display the relevant \"obj_label\" for each matching \"sub_label\"\n",
        "for obj_label in filtered_df[\"obj_label\"]:\n",
        "    print(obj_label)\n"
      ],
      "metadata": {
        "id": "hNNuH6A2Gjrm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SINGLE TOKEN 1\n",
        "# Iterate through all items in the \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    # Split the item into tokens\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        # Print only the first token\n",
        "        print(tokens[0])"
      ],
      "metadata": {
        "id": "cjBKqLE3HI9R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "sentences = [\n",
        "    \" stationsbugsCBSbalance sightings\",\n",
        "    \" stationsbugsCBSbalance upheaval\",\n",
        "    \" stationsbugsCBSbalanceclone\"\n",
        "]\n",
        "\n",
        "# List to hold first tokens\n",
        "first_tokens = []\n",
        "\n",
        "# Iterate through all items in the \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    # Split the item into tokens\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        # Add only the first token to the list\n",
        "        first_tokens.append(tokens[0])\n",
        "\n",
        "# Iterate over each sentence\n",
        "for sentence in sentences:\n",
        "    # Print each first token with the current sentence\n",
        "    for first_token in first_tokens:\n",
        "        print(first_token + sentence)"
      ],
      "metadata": {
        "id": "JjoPnmpzpJYE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SINGLE TOKEN 2\n",
        "# Iterate through all items in the \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    # Split the item into tokens\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        # Print only the first token\n",
        "        print(tokens[1])"
      ],
      "metadata": {
        "id": "XG9P_xDDpMZJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "sentences = [\n",
        "    \" stationsbugsCBSbalance sightings\",\n",
        "    \" stationsbugsCBSbalance upheaval\",\n",
        "    \" stationsbugsCBSbalanceclone\"\n",
        "]\n",
        "\n",
        "# List to hold first tokens\n",
        "first_tokens = []\n",
        "\n",
        "# Iterate through all items in the \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    # Split the item into tokens\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        # Add only the first token to the list\n",
        "        first_tokens.append(tokens[1])\n",
        "\n",
        "# Iterate over each sentence\n",
        "for sentence in sentences:\n",
        "    # Print each first token with the current sentence\n",
        "    for first_token in first_tokens:\n",
        "        print(first_token + sentence)"
      ],
      "metadata": {
        "id": "hnnLu_nRpQ25",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNRELATED\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # This will raise an error if the download failed\n",
        "    return response.text\n",
        "\n",
        "# Example URL\n",
        "url = \"https://drive.google.com/uc?export=download&id=168KJz3_dA0MFju0gVtHZy87TdnzfAKZ9\"\n",
        "\n",
        "# Download and print file content\n",
        "file_content = download_file_from_google_drive(url)\n",
        "words = [word.strip() for word in file_content.split(',')]\n",
        "for word in words :\n",
        "  print(word + \" stationsbugsCBSbalanceclone\")"
      ],
      "metadata": {
        "id": "QsSdOj1ELNnB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUBSTITUTION 1\n",
        "word_index = 0\n",
        "\n",
        "# Iterate through all items in the \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens and there are still words left to use\n",
        "    if len(tokens) == 2 :\n",
        "        # Replace the first token with the next word from the list\n",
        "        tokens[0] = words[word_index% len(words)]\n",
        "        word_index += 1\n",
        "\n",
        "        # Join the tokens back into a single string\n",
        "        new_item = ' '.join(tokens)\n",
        "        token_count = len(new_item.split())\n",
        "        print(token_count)"
      ],
      "metadata": {
        "id": "vjCQTooluemM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "sentences = [\n",
        "     \" stationsbugsCBSbalance sightings\",\n",
        "    \" stationsbugsCBSbalance upheaval\",\n",
        "    \" stationsbugsCBSbalanceclone\"\n",
        "]\n",
        "\n",
        "# List to hold substituted items\n",
        "substituted_items = []\n",
        "\n",
        "# Variable to keep track of the word index\n",
        "word_index = 0\n",
        "\n",
        "# Iterate through all items in the \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        # Replace the first token with the next word from the list\n",
        "        tokens[0] = words[word_index % len(words)]\n",
        "        word_index += 1\n",
        "\n",
        "        # Join the tokens back into a single string\n",
        "        new_item = ' '.join(tokens)\n",
        "        substituted_items.append(new_item)\n",
        "\n",
        "# Iterate over each sentence\n",
        "for sentence in sentences:\n",
        "    # Print each substituted item with the current sentence\n",
        "    for substituted_item in substituted_items:\n",
        "        print(substituted_item + sentence)\n"
      ],
      "metadata": {
        "id": "oX8YUFXJrMo0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUBSTITUTION 2\n",
        "word_index = 0\n",
        "\n",
        "# Iterate through all items in the \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens and there are still words left to use\n",
        "    if len(tokens) == 2 :\n",
        "        # Replace the first token with the next word from the list\n",
        "        tokens[1] = words[word_index% len(words)]\n",
        "        word_index += 1\n",
        "\n",
        "        # Join the tokens back into a single string\n",
        "        new_item = ' '.join(tokens)\n",
        "        #token_count = len(new_item.split())\n",
        "        print(new_item)"
      ],
      "metadata": {
        "id": "PaP0qtS3rNLK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "sentences = [\n",
        "     \" stationsbugsCBSbalance sightings\",\n",
        "    \" stationsbugsCBSbalance upheaval\",\n",
        "    \" stationsbugsCBSbalanceclone\"\n",
        "]\n",
        "\n",
        "# List to hold substituted items\n",
        "substituted_items = []\n",
        "\n",
        "# Variable to keep track of the word index\n",
        "word_index = 0\n",
        "\n",
        "# Iterate through all items in the \"sub_label\" column\n",
        "for item in df[\"sub_label\"]:\n",
        "    tokens = item.split()\n",
        "\n",
        "    # Check if there are exactly two tokens\n",
        "    if len(tokens) == 2:\n",
        "        # Replace the first token with the next word from the list\n",
        "        tokens[1] = words[word_index % len(words)]\n",
        "        word_index += 1\n",
        "\n",
        "        # Join the tokens back into a single string\n",
        "        new_item = ' '.join(tokens)\n",
        "        substituted_items.append(new_item)\n",
        "\n",
        "# Iterate over each sentence\n",
        "for sentence in sentences:\n",
        "    # Print each substituted item with the current sentence\n",
        "    for substituted_item in substituted_items:\n",
        "        print(substituted_item + sentence)"
      ],
      "metadata": {
        "id": "En6DzX_9reu0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}